{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705a4e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the required packages and libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer as sno\n",
    "setofstopwords=set(stopwords.words('english'))\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a795a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connecting to the sqlite database\n",
    "con= sqlite3.connect(r'C:\\Users\\harit\\Downloads\\AmazonFoodReviews\\Data\\database.sqlite')\n",
    "# delete the reviews with th score 3 as it is average\n",
    "data= pd.read_sql_query('''\n",
    "SELECT * FROM Reviews WHERE Score!=3\n",
    "''',con)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0db761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classify the score values based on the score > 3 or not as positive and negative\n",
    "data['Score']=data['Score'].map(lambda x:'Positive' if x>3 else 'Negative')\n",
    "# sort the data based on product ID\n",
    "sorteddata= data.sort_values('ProductId',axis=0)\n",
    "# dropping the duplicates\n",
    "finaldata= sorteddata.drop_duplicates(subset={'UserId','ProfileName',\\\n",
    "        'Time','Text'}, keep='first',inplace=False)\n",
    "finaldata= finaldata[finaldata['HelpfulnessNumerator'] <= finaldata['HelpfulnessDenominator']]\n",
    "data= finaldata.sort_values('Time',axis=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e0fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(sentance): \n",
    "    cleanedData= re.sub(re.compile('<.*?>'),' ',sentance)\n",
    "    return cleanedData\n",
    "#function for removing punctuations chars\n",
    "def cleanpunc(sent):\n",
    "    cleanedData= re.sub(r'[?|!|\\'|\"|#]',r'',sent)\n",
    "    cleanedData= re.sub(r'[.|,|)|(|\\|/]',r'',sent)\n",
    "    return cleanedData\n",
    "snowstemmer= sno('english')\n",
    "\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "for sent in data['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\n",
    "    sent=cleanhtml(sent) # remove HTMl tags\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                s=(snowstemmer.stem(cleaned_words.lower())).encode('utf8')\n",
    "                filtered_sentence.append(s)\n",
    "                if(data['Score'].values)[i] =='Positive':\n",
    "                    all_positive_words.append(s)\n",
    "                if(data['Score'].values)[i] =='Negative':\n",
    "                    all_negative_words.append(s)\n",
    "            else:\n",
    "                continue\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    final_string.append(str1)\n",
    "\n",
    "# storing data till now\n",
    "data['CleanedText']=final_string \n",
    "#adding a column of CleanedText which displays the data after pre-processing of the review \n",
    "data['CleanedText']=data['CleanedText'].str.decode(\"utf-8\")\n",
    "    # store final table into an SQlLite table for future.\n",
    "con = sqlite3.connect('cleanedTextData.sqlite')\n",
    "c=con.cursor()\n",
    "con.text_factory = str\n",
    "data.to_sql('Reviews', con,  schema=None, if_exists='replace', \\\n",
    "        index=True, index_label=None, chunksize=None, dtype=None)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f2f7a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>this witti littl book make son laugh loud reci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138683</th>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "      <td>940809600</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "      <td>can rememb see the show when air televis year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417839</th>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "      <td>beetlejuic well written movi everyth about fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346055</th>\n",
       "      <td>374359</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "      <td>944438400</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>A twist of rumplestiskin captured on film, sta...</td>\n",
       "      <td>twist rumplestiskin captur film star michael k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417838</th>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>946857600</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
       "      <td>beetlejuic excel and funni movi keaton hilari ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId               ProfileName  \\\n",
       "138706  150524  0006641040   ACITT7DI6IDDL           shari zychinski   \n",
       "138683  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "417839  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
       "346055  374359  B00004CI84  A344SMIA5JECGM           Vincent P. Ross   \n",
       "417838  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator     Score       Time  \\\n",
       "138706                     0                       0  Positive  939340800   \n",
       "138683                     2                       2  Positive  940809600   \n",
       "417839                     0                       0  Positive  944092800   \n",
       "346055                     1                       2  Positive  944438400   \n",
       "417838                     0                       0  Positive  946857600   \n",
       "\n",
       "                                                  Summary  \\\n",
       "138706                          EVERY book is educational   \n",
       "138683  This whole series is great way to spend time w...   \n",
       "417839                               Entertainingl Funny!   \n",
       "346055                            A modern day fairy tale   \n",
       "417838                                         FANTASTIC!   \n",
       "\n",
       "                                                     Text  \\\n",
       "138706  this witty little book makes my son laugh at l...   \n",
       "138683  I can remember seeing the show when it aired o...   \n",
       "417839  Beetlejuice is a well written movie ..... ever...   \n",
       "346055  A twist of rumplestiskin captured on film, sta...   \n",
       "417838  Beetlejuice is an excellent and funny movie. K...   \n",
       "\n",
       "                                              CleanedText  \n",
       "138706  this witti littl book make son laugh loud reci...  \n",
       "138683  can rememb see the show when air televis year ...  \n",
       "417839  beetlejuic well written movi everyth about fro...  \n",
       "346055  twist rumplestiskin captur film star michael k...  \n",
       "417838  beetlejuic excel and funni movi keaton hilari ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32460881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50f5df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this confect that has been around few centuri light pillowi citrus gelatin with nut this case filbert and cut into tini squar and then liber coat with powder sugar and tini mouth heaven not too chewi and veri flavor high recommend this yummi treat you are familiar with the stori lion the witch and the this the treat that seduc edmund into sell out his brother and sister the witch'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CleanedText'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d46da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the intermediate data in pickle files\n",
    "import pickle\n",
    "\n",
    "data.to_pickle(\"datafile.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5cb899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting some partof the data with the textand score and excluding the other columns\n",
    "Data= data[:100000]\n",
    "Data= Data[['CleanedText','Score']]\n",
    "Data['Score']= Data['Score'].map(lambda x:1 if x=='Positive' else 0)\n",
    "Data_x= Data['CleanedText']\n",
    "Data_y= Data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a775d0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        this witti littl book make son laugh loud reci...\n",
       "1        can rememb see the show when air televis year ...\n",
       "2        beetlejuic well written movi everyth about fro...\n",
       "3        twist rumplestiskin captur film star michael k...\n",
       "4        beetlejuic excel and funni movi keaton hilari ...\n",
       "                               ...                        \n",
       "99995    this veri tasti protein shake and give you nic...\n",
       "99996    this the best tast oliv and healthi for you to...\n",
       "99997    sack melitta coffe label fine grind blanc noir...\n",
       "99998    final babi food that tast love that great prot...\n",
       "99999    first ventur salt and happi bought this and sa...\n",
       "Name: CleanedText, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_x.index= [i for i in range(0,10**5)]\n",
    "Data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50618d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting all words in single list\n",
    "list_= []\n",
    "for i in Data_x:\n",
    "    list_ += i\n",
    "list_= ''.join(list_)\n",
    "allWords=list_.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6c21769",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab= set(allWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40e35d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list= list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cf8bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency dictionary\n",
    "freq_dict= {}\n",
    "for word in vocabulary_list:\n",
    "    freq_dict[word]= allWords.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e0db2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blehthis': 1,\n",
       " 'organall': 1,\n",
       " 'bettermani': 1,\n",
       " 'themomg': 1,\n",
       " 'dusk': 1,\n",
       " 'shorteningbut': 1,\n",
       " 'helpbeen': 1,\n",
       " 'tilth': 2,\n",
       " 'supermartpurchas': 1,\n",
       " 'flavornice': 2,\n",
       " 'wafflveri': 1,\n",
       " 'vanillacolumbian': 1,\n",
       " 'manythi': 1,\n",
       " 'preservthis': 4,\n",
       " 'foundgiven': 1,\n",
       " 'adhespoor': 1,\n",
       " 'teethreceiv': 1,\n",
       " 'rightway': 1,\n",
       " 'ourfar': 1,\n",
       " 'gumballsw': 1,\n",
       " 'berrri': 3,\n",
       " 'lunchgreat': 3,\n",
       " 'stanolssterol': 1,\n",
       " 'sugarabsolut': 1,\n",
       " 'crockeri': 9,\n",
       " 'chucklforget': 1,\n",
       " 'likeoften': 1,\n",
       " 'dervish': 1,\n",
       " 'wasmin': 1,\n",
       " 'strongeasi': 1,\n",
       " 'extol': 5,\n",
       " 'whiski': 12,\n",
       " 'stillalon': 1,\n",
       " 'invoic': 16,\n",
       " 'committ': 3,\n",
       " 'wrongsplenda': 1,\n",
       " 'breakfastour': 1,\n",
       " 'nugent': 1,\n",
       " 'spinach': 201,\n",
       " 'readithis': 4,\n",
       " 'boar': 2,\n",
       " 'burmathis': 1,\n",
       " 'againwhatev': 1,\n",
       " 'comereceiv': 1,\n",
       " 'rainbow': 19,\n",
       " 'amazonwell': 3,\n",
       " 'themonli': 1,\n",
       " 'betterso': 1,\n",
       " 'madwoman': 2,\n",
       " 'bracer': 1,\n",
       " 'christmalizano': 1,\n",
       " 'mealbut': 1,\n",
       " 'oounc': 1,\n",
       " 'wheatitem': 1,\n",
       " 'icethis': 11,\n",
       " 'davisonnew': 1,\n",
       " 'panthese': 1,\n",
       " 'variationson': 1,\n",
       " 'largthis': 3,\n",
       " 'fruitat': 2,\n",
       " 'greathard': 1,\n",
       " 'zappo': 1,\n",
       " 'foodrecent': 1,\n",
       " 'horseradich': 1,\n",
       " 'breadafter': 2,\n",
       " 'defent': 1,\n",
       " 'celebrhad': 1,\n",
       " 'wend': 1,\n",
       " 'storesi': 4,\n",
       " 'herbspic': 1,\n",
       " 'thisfew': 1,\n",
       " 'sameit': 2,\n",
       " 'presenttake': 1,\n",
       " 'owndon': 1,\n",
       " 'gravyi': 1,\n",
       " 'storeyear': 1,\n",
       " 'cavendish': 3,\n",
       " 'enjoyswear': 1,\n",
       " 'stupefi': 1,\n",
       " 'stuffawesom': 1,\n",
       " 'twistthis': 2,\n",
       " 'blueberriall': 1,\n",
       " 'swimsuit': 1,\n",
       " 'pouringscoop': 1,\n",
       " 'ouralthough': 1,\n",
       " 'gori': 9,\n",
       " 'casepassov': 1,\n",
       " 'charlott': 4,\n",
       " 'postscript': 1,\n",
       " 'chewerspecial': 1,\n",
       " 'recommendmani': 1,\n",
       " 'foodspet': 1,\n",
       " 'catones': 1,\n",
       " 'masteri': 1,\n",
       " 'thatthought': 2,\n",
       " 'fallback': 2,\n",
       " 'undigest': 11,\n",
       " 'turkeygreat': 1,\n",
       " 'roasttoo': 1,\n",
       " 'knowal': 1,\n",
       " 'companygristed': 1,\n",
       " 'batchlove': 1,\n",
       " 'hullabaloo': 1,\n",
       " 'yeargave': 1,\n",
       " 'warmth': 33,\n",
       " 'chilledfor': 1,\n",
       " 'beanjust': 2,\n",
       " 'notciabl': 1,\n",
       " 'seafoodthese': 1,\n",
       " 'cookiesyellow': 1,\n",
       " 'packetenjoy': 1,\n",
       " 'cakewhole': 1,\n",
       " 'sniffer': 2,\n",
       " 'youall': 7,\n",
       " 'productr': 1,\n",
       " 'grandmothersmoth': 1,\n",
       " 'greatconveni': 1,\n",
       " 'catalyz': 2,\n",
       " 'kitchenhave': 3,\n",
       " 'andgo': 1,\n",
       " 'amerasian': 1,\n",
       " 'cheerwin': 1,\n",
       " 'mil': 4,\n",
       " 'aregreen': 1,\n",
       " 'panera': 2,\n",
       " 'papercardboard': 1,\n",
       " 'monthlocal': 1,\n",
       " 'croatian': 3,\n",
       " 'dreamlol': 1,\n",
       " 'competitorhave': 1,\n",
       " 'freemost': 1,\n",
       " 'waterwolfgang': 1,\n",
       " 'congeni': 1,\n",
       " 'acv': 26,\n",
       " 'resutl': 2,\n",
       " 'supplment': 2,\n",
       " 'jamsjelli': 2,\n",
       " 'grillgood': 2,\n",
       " 'vinegarywateri': 1,\n",
       " 'incredtri': 1,\n",
       " 'beerpicki': 1,\n",
       " 'pea': 536,\n",
       " 'brule': 60,\n",
       " 'loveconveni': 1,\n",
       " 'justone': 1,\n",
       " 'arthrti': 1,\n",
       " 'ronni': 1,\n",
       " 'harmlesstri': 1,\n",
       " 'lovedlov': 1,\n",
       " 'beforget': 1,\n",
       " 'mueslithe': 1,\n",
       " 'stomachembarrass': 1,\n",
       " 'wasteread': 1,\n",
       " 'fragant': 2,\n",
       " 'tinkyadafamili': 1,\n",
       " 'useri': 1,\n",
       " 'dcide': 1,\n",
       " 'freebought': 4,\n",
       " 'hairbo': 2,\n",
       " 'figger': 1,\n",
       " 'teahob': 1,\n",
       " 'withworri': 1,\n",
       " 'licoricdoc': 1,\n",
       " 'thoughth': 4,\n",
       " 'productclosest': 1,\n",
       " 'rightread': 4,\n",
       " 'singleservecoffeecomrare': 1,\n",
       " 'buyitem': 2,\n",
       " 'gelatinno': 1,\n",
       " 'teapotwas': 1,\n",
       " 'awesomealmost': 1,\n",
       " 'thoughflavor': 1,\n",
       " 'texturecrunchi': 1,\n",
       " 'reasonwell': 1,\n",
       " 'hotbase': 1,\n",
       " 'handhuge': 1,\n",
       " 'shap': 2,\n",
       " 'knowedg': 1,\n",
       " 'charg': 700,\n",
       " 'saltcrazi': 1,\n",
       " 'flavorsparkl': 1,\n",
       " 'eattwine': 1,\n",
       " 'glamor': 2,\n",
       " 'freewheat': 5,\n",
       " 'throwsure': 1,\n",
       " 'peve': 1,\n",
       " 'dietkarl': 1,\n",
       " 'intim': 5,\n",
       " 'mornthe': 12,\n",
       " 'gianduja': 3,\n",
       " 'pyrotechn': 1,\n",
       " 'kennedi': 3,\n",
       " 'silicoallumin': 1,\n",
       " 'crumblhave': 1,\n",
       " 'candikeep': 1,\n",
       " 'compplain': 1,\n",
       " 'disguis': 50,\n",
       " 'blandpetit': 1,\n",
       " 'antioxidihav': 1,\n",
       " 'trustcoconut': 1,\n",
       " 'reschedul': 1,\n",
       " 'storigreat': 1,\n",
       " 'transactfar': 1,\n",
       " 'cardand': 2,\n",
       " 'cerealstart': 1,\n",
       " 'erupt': 6,\n",
       " 'marilyn': 2,\n",
       " 'wkale': 1,\n",
       " 'andsun': 1,\n",
       " 'thisship': 1,\n",
       " 'crumpetuse': 1,\n",
       " 'immead': 1,\n",
       " 'infoi': 1,\n",
       " 'cheeselik': 1,\n",
       " 'sixpack': 3,\n",
       " 'withexpect': 1,\n",
       " 'themtoo': 4,\n",
       " 'texturefin': 1,\n",
       " 'expat': 5,\n",
       " 'cakeson': 1,\n",
       " 'focuss': 1,\n",
       " 'powfer': 1,\n",
       " 'partak': 27,\n",
       " 'tool': 107,\n",
       " 'shihkanes': 1,\n",
       " 'safflow': 51,\n",
       " 'reastyl': 1,\n",
       " 'somenew': 1,\n",
       " 'accolad': 5,\n",
       " 'toysproduct': 1,\n",
       " 'entomolog': 1,\n",
       " 'thyroidadren': 1,\n",
       " 'dogma': 2,\n",
       " 'pourproduct': 1,\n",
       " 'tributari': 1,\n",
       " 'comerich': 1,\n",
       " 'roastingflavor': 1,\n",
       " 'curd': 30,\n",
       " 'ammoni': 2,\n",
       " 'dugout': 1,\n",
       " 'itemth': 1,\n",
       " 'ramenpart': 1,\n",
       " 'henderson': 1,\n",
       " 'storyw': 1,\n",
       " 'andrealiz': 2,\n",
       " 'referesh': 1,\n",
       " 'swarm': 9,\n",
       " 'availzsweet': 1,\n",
       " 'fuelreseach': 1,\n",
       " 'understatgrandson': 1,\n",
       " 'wherehow': 2,\n",
       " 'grillhave': 1,\n",
       " 'starspg': 1,\n",
       " 'onehard': 1,\n",
       " 'licoricfor': 1,\n",
       " 'pinciottiman': 1,\n",
       " 'bewild': 1,\n",
       " 'grindingtampingclean': 1,\n",
       " 'ibuprofen': 7,\n",
       " 'oncentr': 1,\n",
       " 'roundthe': 2,\n",
       " 'sandwichcame': 1,\n",
       " 'oooop': 1,\n",
       " 'iceorder': 2,\n",
       " 'shinithis': 1,\n",
       " 'fromhave': 8,\n",
       " 'andexpect': 2,\n",
       " 'futurjust': 2,\n",
       " 'senticosus': 1,\n",
       " 'firecrack': 1,\n",
       " 'chewteeth': 1,\n",
       " 'sweetregular': 1,\n",
       " 'centano': 1,\n",
       " 'casetea': 1,\n",
       " 'throughthank': 1,\n",
       " 'roomindoor': 1,\n",
       " 'jumpstart': 4,\n",
       " 'findprefer': 1,\n",
       " 'thisonc': 2,\n",
       " 'coldgreat': 2,\n",
       " 'onlinonli': 1,\n",
       " 'sugarother': 1,\n",
       " 'sealham': 1,\n",
       " 'mdx': 1,\n",
       " 'surpristoo': 1,\n",
       " 'delieious': 1,\n",
       " 'himselfhave': 1,\n",
       " 'houus': 1,\n",
       " 'wrongming': 1,\n",
       " 'hugethese': 1,\n",
       " 'amex': 1,\n",
       " 'hilton': 3,\n",
       " 'oyu': 1,\n",
       " 'overallw': 1,\n",
       " 'caloritwo': 1,\n",
       " 'firethis': 2,\n",
       " 'sconestook': 1,\n",
       " 'paprika': 130,\n",
       " 'picturthese': 1,\n",
       " 'demor': 1,\n",
       " 'freshnessthey': 1,\n",
       " 'commentthis': 2,\n",
       " 'boxerbulldog': 1,\n",
       " 'koolaid': 14,\n",
       " 'mina': 1,\n",
       " 'pesticid': 110,\n",
       " 'mistreat': 1,\n",
       " 'emelmet': 1,\n",
       " 'ramotsw': 2,\n",
       " 'cryor': 1,\n",
       " 'affecionado': 1,\n",
       " 'differdog': 1,\n",
       " 'springdelici': 1,\n",
       " 'wir': 1,\n",
       " 'pastpractic': 1,\n",
       " 'idestruct': 1,\n",
       " 'gerbera': 2,\n",
       " 'jamica': 1,\n",
       " 'hotbe': 1,\n",
       " 'teasp': 2,\n",
       " 'agenda': 10,\n",
       " 'flavorul': 2,\n",
       " 'foodsinst': 1,\n",
       " 'boxuse': 7,\n",
       " 'lowney': 1,\n",
       " 'fetish': 3,\n",
       " 'flavorentir': 1,\n",
       " 'blueberrihave': 1,\n",
       " 'didgreat': 3,\n",
       " 'vaughn': 1,\n",
       " 'dbl': 1,\n",
       " 'minedamn': 1,\n",
       " 'disappointwork': 2,\n",
       " 'kahvesi': 2,\n",
       " 'foundtast': 1,\n",
       " 'overheard': 3,\n",
       " 'raddichio': 1,\n",
       " 'cheaperorder': 1,\n",
       " 'fanthis': 8,\n",
       " 'groeceri': 2,\n",
       " 'bride': 7,\n",
       " 'preferthe': 1,\n",
       " 'povov': 1,\n",
       " 'dietbase': 1,\n",
       " 'saidenjoy': 1,\n",
       " 'downbe': 1,\n",
       " 'loveeven': 2,\n",
       " 'vanalla': 1,\n",
       " 'rightthey': 1,\n",
       " 'glutencat': 1,\n",
       " 'knowlove': 3,\n",
       " 'packageat': 1,\n",
       " 'yicki': 3,\n",
       " 'ridiculousov': 1,\n",
       " 'costfiddyment': 1,\n",
       " 'district': 15,\n",
       " 'fingr': 1,\n",
       " 'everyonthere': 1,\n",
       " 'amarena': 2,\n",
       " 'themlavazza': 2,\n",
       " 'sheperdretrieverlabrador': 1,\n",
       " 'storethen': 1,\n",
       " 'togeththe': 1,\n",
       " 'scanti': 1,\n",
       " 'myriad': 23,\n",
       " 'implement': 8,\n",
       " 'sandwhichesand': 1,\n",
       " 'austinhave': 1,\n",
       " 'poblana': 1,\n",
       " 'teem': 3,\n",
       " 'bootorgan': 1,\n",
       " 'splinterthese': 1,\n",
       " 'cocco': 2,\n",
       " 'fantastkid': 1,\n",
       " 'chewgreat': 2,\n",
       " 'bargainafter': 1,\n",
       " 'anicheesi': 1,\n",
       " 'backchocol': 1,\n",
       " 'laryngopharyng': 1,\n",
       " 'vitacoco': 27,\n",
       " 'doneexpect': 1,\n",
       " 'mycup': 1,\n",
       " 'sanalac': 5,\n",
       " 'allwish': 1,\n",
       " 'vend': 117,\n",
       " 'shortbread': 160,\n",
       " 'open': 3860,\n",
       " 'havahart': 4,\n",
       " 'kountri': 2,\n",
       " 'leastginger': 1,\n",
       " 'hawk': 5,\n",
       " 'brothfate': 1,\n",
       " 'bettercook': 1,\n",
       " 'openther': 1,\n",
       " 'drinkgreat': 3,\n",
       " 'giftpocki': 1,\n",
       " 'againhuski': 1,\n",
       " 'pouchpoint': 2,\n",
       " 'saylong': 1,\n",
       " 'subset': 2,\n",
       " 'avoderm': 19,\n",
       " 'containhave': 3,\n",
       " 'naga': 5,\n",
       " 'westchest': 1,\n",
       " 'cerealokay': 2,\n",
       " 'cursori': 2,\n",
       " 'greater': 106,\n",
       " 'gano': 4,\n",
       " 'maybought': 1,\n",
       " 'morefor': 1,\n",
       " 'kapaskawho': 1,\n",
       " 'smokesburn': 2,\n",
       " 'mcvayoveral': 1,\n",
       " 'sharehad': 2,\n",
       " 'itoo': 1,\n",
       " 'interimkitti': 1,\n",
       " 'vodkathe': 2,\n",
       " 'bellow': 1,\n",
       " 'housexcel': 1,\n",
       " 'valujust': 1,\n",
       " 'againanchor': 1,\n",
       " 'benefitcould': 1,\n",
       " 'eastthese': 1,\n",
       " 'chipsetc': 1,\n",
       " 'otherbarilla': 1,\n",
       " 'productbusi': 1,\n",
       " 'remembboth': 1,\n",
       " 'dishwas': 3,\n",
       " 'defec': 4,\n",
       " 'waterreceiv': 1,\n",
       " 'satisfithink': 1,\n",
       " 'vinni': 1,\n",
       " 'mac': 474,\n",
       " 'replacnice': 1,\n",
       " 'indecannot': 1,\n",
       " 'modem': 3,\n",
       " 'amznvegetarian': 1,\n",
       " 'osem': 8,\n",
       " 'energneed': 1,\n",
       " 'tripwhen': 1,\n",
       " 'fillhad': 2,\n",
       " 'stepfath': 3,\n",
       " 'maththis': 1,\n",
       " 'downmi': 1,\n",
       " 'beefcan': 1,\n",
       " 'dey': 1,\n",
       " 'partitake': 1,\n",
       " 'normalcithis': 1,\n",
       " 'loverrecent': 1,\n",
       " 'formay': 2,\n",
       " 'squirrelabout': 1,\n",
       " 'housew': 1,\n",
       " 'tinfirst': 1,\n",
       " 'chewjust': 1,\n",
       " 'betterlittl': 2,\n",
       " 'dayhard': 1,\n",
       " 'jeff': 14,\n",
       " 'terrierpug': 1,\n",
       " 'mixspent': 1,\n",
       " 'weekprobabl': 1,\n",
       " 'turkeyi': 1,\n",
       " 'bagstot': 1,\n",
       " 'guion': 1,\n",
       " 'waistlinlive': 1,\n",
       " 'soya': 15,\n",
       " 'yourselfwhile': 1,\n",
       " 'ihop': 7,\n",
       " 'noisbought': 1,\n",
       " 'forkliftwould': 1,\n",
       " 'appletis': 2,\n",
       " 'treatbest': 2,\n",
       " 'anymorrabbit': 1,\n",
       " 'grainsand': 1,\n",
       " 'kaya': 1,\n",
       " 'awhunger': 1,\n",
       " 'goodnatur': 1,\n",
       " 'aprovechthis': 1,\n",
       " 'fillbuy': 1,\n",
       " 'vacatdog': 1,\n",
       " 'usehave': 15,\n",
       " 'concoctthis': 1,\n",
       " 'creosot': 1,\n",
       " 'disadvantag': 24,\n",
       " 'outsmart': 3,\n",
       " 'reviewchihuahua': 1,\n",
       " 'bankfresh': 1,\n",
       " 'unstabil': 2,\n",
       " 'middlesay': 1,\n",
       " 'morehusband': 3,\n",
       " 'fungicidaldiseas': 1,\n",
       " 'delciousthis': 1,\n",
       " 'drilast': 1,\n",
       " 'placehusband': 1,\n",
       " 'joyvah': 2,\n",
       " 'braver': 1,\n",
       " 'emulsifi': 29,\n",
       " 'accept': 340,\n",
       " 'bowllive': 1,\n",
       " 'sourz': 1,\n",
       " 'previousclassic': 1,\n",
       " 'lettucthis': 2,\n",
       " 'steril': 20,\n",
       " 'housei': 2,\n",
       " 'groan': 8,\n",
       " 'kestekidi': 4,\n",
       " 'milkknow': 2,\n",
       " 'morselcolumbus': 1,\n",
       " 'buddijust': 1,\n",
       " 'foodalmost': 1,\n",
       " 'denslove': 1,\n",
       " 'sellersingl': 1,\n",
       " 'brandbear': 1,\n",
       " 'unenrich': 1,\n",
       " 'conspir': 2,\n",
       " 'conisseur': 1,\n",
       " 'loafuse': 1,\n",
       " 'heremust': 1,\n",
       " 'opel': 1,\n",
       " 'pillowthis': 1,\n",
       " 'personenjoy': 1,\n",
       " 'bedtimbeen': 1,\n",
       " 'molassahad': 1,\n",
       " 'petit': 166,\n",
       " 'freshpak': 7,\n",
       " 'cupboardi': 1,\n",
       " 'caseorbit': 1,\n",
       " 'playstat': 1,\n",
       " 'crefu': 1,\n",
       " 'multiberri': 1,\n",
       " 'countfar': 1,\n",
       " 'coffethank': 1,\n",
       " 'concoctionso': 1,\n",
       " 'malaseb': 1,\n",
       " 'meanever': 1,\n",
       " 'coffeever': 1,\n",
       " 'pomboston': 1,\n",
       " 'productphilli': 1,\n",
       " 'moisturquot': 1,\n",
       " 'goodother': 2,\n",
       " 'buttergreat': 2,\n",
       " 'positwas': 1,\n",
       " 'goi': 4,\n",
       " 'tastyou': 22,\n",
       " 'everbett': 1,\n",
       " 'septembthe': 1,\n",
       " 'receptthis': 1,\n",
       " 'carrotsand': 1,\n",
       " 'satir': 1,\n",
       " 'dealamazonhurray': 1,\n",
       " 'choicyear': 1,\n",
       " 'positon': 1,\n",
       " 'buggerusual': 1,\n",
       " 'bartlet': 2,\n",
       " 'macrobiologist': 1,\n",
       " 'mousthis': 1,\n",
       " 'chaser': 8,\n",
       " 'macchiatoanyday': 1,\n",
       " 'multibl': 2,\n",
       " 'impressa': 1,\n",
       " 'ashlim': 1,\n",
       " 'mesawife': 1,\n",
       " 'greatloaf': 1,\n",
       " 'fastrealli': 2,\n",
       " 'shopmost': 1,\n",
       " 'catchfamili': 1,\n",
       " 'deliveriagre': 1,\n",
       " 'bittertri': 1,\n",
       " 'vidalia': 5,\n",
       " 'steviahello': 1,\n",
       " 'frugal': 23,\n",
       " 'poctur': 1,\n",
       " 'cort': 1,\n",
       " 'pipper': 2,\n",
       " 'websitdog': 1,\n",
       " 'orwhat': 1,\n",
       " 'thekashi': 1,\n",
       " 'guanyl': 4,\n",
       " 'prettiwas': 2,\n",
       " 'thansad': 1,\n",
       " 'thankone': 1,\n",
       " 'baddrank': 1,\n",
       " 'measurgood': 1,\n",
       " 'trailhead': 1,\n",
       " 'herefound': 2,\n",
       " 'catnipi': 1,\n",
       " 'varietiesif': 1,\n",
       " 'christmawhat': 1,\n",
       " 'staplthis': 1,\n",
       " 'everytimbeen': 2,\n",
       " 'schnouzer': 2,\n",
       " 'poeticthis': 1,\n",
       " 'cholesterolabsolut': 1,\n",
       " 'recommenddaughter': 4,\n",
       " 'cinnamoncardamom': 1,\n",
       " 'showerth': 1,\n",
       " 'amazonyou': 17,\n",
       " 'buyrecent': 3,\n",
       " 'woul': 3,\n",
       " 'waistlinthe': 1,\n",
       " 'flavorhard': 3,\n",
       " 'definwhole': 1,\n",
       " 'etcnot': 2,\n",
       " 'coffeeshopdrink': 1,\n",
       " 'oneoldest': 1,\n",
       " 'thisprice': 1,\n",
       " 'pastait': 1,\n",
       " 'outbreakthis': 1,\n",
       " 'bagbuy': 1,\n",
       " 'evenson': 1,\n",
       " 'fatcarbsoth': 1,\n",
       " 'collegu': 1,\n",
       " 'sweetexcel': 1,\n",
       " 'carter': 4,\n",
       " 'girardeau': 1,\n",
       " 'guilthere': 1,\n",
       " 'bracketquit': 1,\n",
       " 'argentinathis': 1,\n",
       " 'ffxiii': 1,\n",
       " 'yousoak': 1,\n",
       " 'chunkon': 1,\n",
       " 'beforkid': 1,\n",
       " 'grandkid': 75,\n",
       " 'hafterhave': 1,\n",
       " 'quickhand': 1,\n",
       " 'serviccomment': 1,\n",
       " 'fran': 9,\n",
       " 'alternyes': 1,\n",
       " 'arbuckl': 3,\n",
       " 'bloodstreamfor': 1,\n",
       " 'highpleas': 2,\n",
       " 'heartitast': 1,\n",
       " 'blendsent': 1,\n",
       " 'superbsearch': 1,\n",
       " 'cakeor': 1,\n",
       " 'sincour': 1,\n",
       " 'archaic': 1,\n",
       " 'saladnotic': 1,\n",
       " 'nowluckili': 1,\n",
       " 'tankard': 1,\n",
       " 'applebutt': 1,\n",
       " 'luckhave': 1,\n",
       " 'flavoringalso': 1,\n",
       " 'cupokay': 1,\n",
       " 'thisfriend': 3,\n",
       " 'kun': 1,\n",
       " 'purchasfrom': 2,\n",
       " 'arl': 1,\n",
       " 'recipgrew': 1,\n",
       " 'graffiti': 3,\n",
       " 'nyla': 2,\n",
       " 'sandwichor': 1,\n",
       " 'youngerafter': 1,\n",
       " 'calorithink': 1,\n",
       " 'listagre': 1,\n",
       " 'savelove': 7,\n",
       " 'fourhad': 1,\n",
       " 'excelmorton': 1,\n",
       " 'highgave': 2,\n",
       " 'favorithershey': 1,\n",
       " 'meanign': 1,\n",
       " 'agre': 873,\n",
       " 'soupgrandchildren': 1,\n",
       " 'dogthank': 1,\n",
       " 'occaskeep': 1,\n",
       " 'edenhave': 1,\n",
       " 'convinc': 197,\n",
       " 'particularili': 3,\n",
       " 'kierland': 1,\n",
       " 'wonder': 6120,\n",
       " 'uniniti': 15,\n",
       " 'othermayb': 1,\n",
       " 'legendari': 13,\n",
       " 'bge': 1,\n",
       " 'andour': 4,\n",
       " 'mfgs': 1,\n",
       " 'imagain': 2,\n",
       " 'familifamili': 1,\n",
       " 'ipa': 1,\n",
       " 'thmwonder': 1,\n",
       " 'pagewean': 1,\n",
       " 'brickshallot': 1,\n",
       " 'goodcooki': 1,\n",
       " 'buzztri': 1,\n",
       " 'pellicl': 2,\n",
       " 'agon': 3,\n",
       " 'treeand': 1,\n",
       " 'waitdassant': 1,\n",
       " 'runniuse': 1,\n",
       " 'storebig': 1,\n",
       " 'regalo': 1,\n",
       " 'timeweight': 1,\n",
       " 'andexcel': 3,\n",
       " 'snacklive': 1,\n",
       " 'bran': 424,\n",
       " 'seenthank': 1,\n",
       " 'gringofollow': 1,\n",
       " 'haveother': 1,\n",
       " 'realhave': 5,\n",
       " 'bethis': 4,\n",
       " 'whatfell': 1,\n",
       " 'extralow': 1,\n",
       " 'bedbathandbeyond': 1,\n",
       " 'areaunlik': 1,\n",
       " 'lotbole': 1,\n",
       " 'toofifti': 1,\n",
       " 'travelchamomil': 1,\n",
       " 'yearbichon': 1,\n",
       " 'anythingfrom': 1,\n",
       " 'expericousin': 1,\n",
       " 'dietdrank': 1,\n",
       " 'toppingsin': 1,\n",
       " 'chickenad': 1,\n",
       " 'ocur': 2,\n",
       " 'visitng': 1,\n",
       " 'mycodex': 1,\n",
       " 'yumnut': 1,\n",
       " 'muchbuy': 1,\n",
       " 'yourselfpurchas': 1,\n",
       " 'moorjani': 1,\n",
       " 'boxalway': 2,\n",
       " 'shortbreadyhowev': 1,\n",
       " 'qualitiknow': 1,\n",
       " 'redfish': 2,\n",
       " 'oatthis': 2,\n",
       " 'tradeshow': 1,\n",
       " 'onethrough': 1,\n",
       " 'viveveri': 1,\n",
       " 'verifruiti': 1,\n",
       " 'makeboil': 1,\n",
       " 'cabalos': 1,\n",
       " 'tablespoonand': 1,\n",
       " 'peoplhave': 3,\n",
       " 'fructuos': 1,\n",
       " 'planson': 1,\n",
       " 'cuisinlove': 1,\n",
       " 'thechild': 1,\n",
       " 'usew': 1,\n",
       " 'revolutdog': 1,\n",
       " 'crispithese': 1,\n",
       " 'brandwhole': 1,\n",
       " 'wishbon': 4,\n",
       " 'plantingredi': 1,\n",
       " 'matterthis': 1,\n",
       " 'themdecid': 1,\n",
       " 'hitfun': 1,\n",
       " 'packmom': 2,\n",
       " 'davidthis': 1,\n",
       " 'newshave': 1,\n",
       " 'kernelold': 1,\n",
       " 'thetofu': 2,\n",
       " 'christmacrispi': 1,\n",
       " 'royaltiexcel': 1,\n",
       " 'ripgreen': 1,\n",
       " 'dogenough': 1,\n",
       " 'againgreen': 1,\n",
       " 'powderthe': 3,\n",
       " 'dudetri': 1,\n",
       " 'valukid': 1,\n",
       " 'withrossa': 1,\n",
       " 'saladbut': 2,\n",
       " 'diseasthis': 1,\n",
       " 'gentl': 275,\n",
       " 'productextrem': 2,\n",
       " 'woodshop': 1,\n",
       " 'similac': 106,\n",
       " 'outsaf': 1,\n",
       " 'realahead': 1,\n",
       " 'alma': 2,\n",
       " 'busisweet': 1,\n",
       " 'labgreyhound': 1,\n",
       " 'conscienti': 9,\n",
       " 'getcrystal': 1,\n",
       " 'coffelearn': 1,\n",
       " 'befuddl': 1,\n",
       " 'friglike': 1,\n",
       " 'nowimport': 1,\n",
       " 'againhere': 2,\n",
       " 'nugrap': 1,\n",
       " 'cookieat': 2,\n",
       " 'betterwife': 2,\n",
       " 'stepchild': 1,\n",
       " 'brewerbeen': 1,\n",
       " 'starchnon': 1,\n",
       " 'fondor': 2,\n",
       " 'individuorgan': 1,\n",
       " 'comparisonpleas': 1,\n",
       " 'sweetener': 1,\n",
       " 'glutenokay': 1,\n",
       " 'excelnoth': 1,\n",
       " 'icefor': 1,\n",
       " 'bagsso': 1,\n",
       " 'unfortun': 1084,\n",
       " 'reveiwroommat': 1,\n",
       " 'mehorder': 1,\n",
       " 'typewell': 1,\n",
       " 'caloritake': 1,\n",
       " 'scrumptiouspower': 1,\n",
       " 'testhad': 1,\n",
       " 'pks': 6,\n",
       " 'runwas': 1,\n",
       " 'chewerwife': 1,\n",
       " 'roe': 9,\n",
       " 'dryness': 24,\n",
       " 'traysold': 1,\n",
       " 'thatbuddi': 1,\n",
       " 'dealthink': 3,\n",
       " 'spathiphyllum': 1,\n",
       " 'goodwonder': 2,\n",
       " 'bowlthe': 1,\n",
       " 'fervor': 4,\n",
       " 'careclean': 1,\n",
       " 'clearwat': 1,\n",
       " 'packaga': 1,\n",
       " 'trounc': 1,\n",
       " 'weeklive': 1,\n",
       " 'stomachdigest': 1,\n",
       " 'yeeethis': 1,\n",
       " 'tastnever': 7,\n",
       " 'sandwichbumbl': 1,\n",
       " 'cutetri': 1,\n",
       " 'kuer': 1,\n",
       " 'wastag': 3,\n",
       " 'goodbam': 1,\n",
       " 'sizesmal': 1,\n",
       " 'bootdog': 1,\n",
       " 'fingernever': 1,\n",
       " 'packagthis': 30,\n",
       " 'drewfus': 1,\n",
       " 'niceanoth': 1,\n",
       " 'hirehave': 1,\n",
       " 'inexpensivegood': 1,\n",
       " 'secondjablum': 1,\n",
       " 'gft': 1,\n",
       " 'symtom': 3,\n",
       " 'satisfiwhen': 2,\n",
       " 'debri': 17,\n",
       " 'trithere': 3,\n",
       " 'mixturegemischtbut': 1,\n",
       " 'ristto': 1,\n",
       " 'webfirst': 1,\n",
       " 'halifax': 2,\n",
       " 'halfkilo': 1,\n",
       " 'immediwas': 1,\n",
       " 'sweetin': 2,\n",
       " 'recoup': 2,\n",
       " 'somei': 1,\n",
       " 'gumeat': 1,\n",
       " 'guessnot': 1,\n",
       " 'intenselov': 1,\n",
       " 'hedg': 6,\n",
       " 'soonchip': 1,\n",
       " 'traditionsth': 1,\n",
       " 'gardenorigin': 1,\n",
       " 'faractual': 1,\n",
       " 'maletti': 1,\n",
       " 'couldhad': 1,\n",
       " 'varitiesthen': 1,\n",
       " 'kaluha': 3,\n",
       " 'gratest': 1,\n",
       " 'everybodibought': 1,\n",
       " 'goldenroast': 1,\n",
       " 'cookichef': 1,\n",
       " 'gingerfor': 1,\n",
       " 'gifthlaf': 1,\n",
       " 'againsee': 1,\n",
       " 'eitherrealli': 1,\n",
       " 'squishi': 26,\n",
       " 'roastgreat': 1,\n",
       " 'rayson': 1,\n",
       " 'flavorno': 4,\n",
       " 'fooder': 2,\n",
       " 'figsway': 1,\n",
       " 'problempurchas': 1,\n",
       " 'forpath': 1,\n",
       " 'dynomit': 2,\n",
       " 'arrivfirst': 1,\n",
       " 'begg': 1,\n",
       " 'welllong': 1,\n",
       " 'flavorsmocha': 1,\n",
       " 'thinkafter': 1,\n",
       " 'onc': 3241,\n",
       " 'evitar': 1,\n",
       " 'deliverithought': 1,\n",
       " 'rins': 380,\n",
       " 'recommendworld': 2,\n",
       " 'listhas': 1,\n",
       " 'muchdoe': 1,\n",
       " 'chivi': 1,\n",
       " 'trevorproduct': 1,\n",
       " 'okaydaughter': 1,\n",
       " 'kindtri': 1,\n",
       " 'storeokay': 1,\n",
       " 'decript': 1,\n",
       " 'mildweak': 1,\n",
       " 'bronx': 6,\n",
       " 'agatha': 2,\n",
       " 'foresight': 2,\n",
       " 'labshepard': 3,\n",
       " 'hypnot': 10,\n",
       " 'thesearriv': 1,\n",
       " 'marmalad': 107,\n",
       " 'marchsinc': 1,\n",
       " 'haunt': 24,\n",
       " 'fewdisappoint': 1,\n",
       " 'daycontain': 1,\n",
       " 'productlemon': 1,\n",
       " 'goodorgan': 2,\n",
       " 'recommendmrs': 1,\n",
       " 'tear': 253,\n",
       " 'teatasti': 2,\n",
       " 'bluethis': 1,\n",
       " 'baseless': 1,\n",
       " 'sega': 1,\n",
       " 'treattruffl': 1,\n",
       " 'startgood': 1,\n",
       " 'lumpavid': 1,\n",
       " 'trader': 351,\n",
       " 'luxuri': 113,\n",
       " 'harney': 41,\n",
       " 'teabaglove': 1,\n",
       " 'midland': 2,\n",
       " 'yuuuumthe': 1,\n",
       " 'caffinbest': 1,\n",
       " 'weekcaught': 1,\n",
       " 'crackergeneral': 1,\n",
       " 'mouthreceiv': 1,\n",
       " 'bedexcel': 1,\n",
       " 'clickstrong': 1,\n",
       " 'stockhold': 1,\n",
       " 'chewveri': 1,\n",
       " 'kickbut': 1,\n",
       " 'saleone': 1,\n",
       " 'nicknack': 1,\n",
       " 'savepurchas': 2,\n",
       " 'intaklove': 3,\n",
       " 'superlove': 1,\n",
       " 'aftercat': 1,\n",
       " 'rightglad': 2,\n",
       " 'xdate': 1,\n",
       " 'buywife': 1,\n",
       " 'yeastit': 1,\n",
       " 'calorieprotein': 1,\n",
       " 'marjarita': 1,\n",
       " 'margarita': 122,\n",
       " 'intakebag': 1,\n",
       " 'soybeanscom': 1,\n",
       " 'thisdiamond': 1,\n",
       " 'treealreadi': 1,\n",
       " 'aouthwest': 1,\n",
       " 'weighthad': 1,\n",
       " 'infothe': 1,\n",
       " 'sarrano': 1,\n",
       " 'likerealiz': 1,\n",
       " 'poision': 1,\n",
       " 'fasttri': 1,\n",
       " 'toothpastthis': 1,\n",
       " 'defunctyou': 1,\n",
       " 'disappointedand': 1,\n",
       " 'starthis': 45,\n",
       " 'proteinsvitamin': 1,\n",
       " 'themkeep': 3,\n",
       " 'themslev': 2,\n",
       " 'floridaonc': 1,\n",
       " 'themborder': 1,\n",
       " 'snootadd': 1,\n",
       " 'carefulb': 1,\n",
       " 'beforehandthis': 1,\n",
       " 'neurotoxin': 7,\n",
       " 'happenfor': 1,\n",
       " 'foodanyth': 1,\n",
       " 'decisbeen': 1,\n",
       " 'individuali': 1,\n",
       " 'des': 8,\n",
       " 'peatnut': 1,\n",
       " 'theforget': 1,\n",
       " 'timear': 1,\n",
       " 'toofind': 3,\n",
       " 'msgawesom': 1,\n",
       " 'branddrank': 1,\n",
       " 'homead': 7,\n",
       " 'chocolpurchas': 1,\n",
       " 'bitteryou': 3,\n",
       " 'foodbolesd': 1,\n",
       " 'flavorwhi': 1,\n",
       " 'willi': 18,\n",
       " 'chipsthi': 1,\n",
       " 'afterliforder': 1,\n",
       " 'originbengal': 1,\n",
       " 'readigave': 1,\n",
       " 'leaveat': 1,\n",
       " 'shapegrew': 1,\n",
       " 'zenith': 1,\n",
       " 'counta': 1,\n",
       " 'wlmart': 1,\n",
       " 'englanduse': 1,\n",
       " 'tummybowel': 2,\n",
       " 'fixtur': 36,\n",
       " 'xylosweetveri': 1,\n",
       " 'mincat': 1,\n",
       " 'moistlike': 2,\n",
       " ...}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "590f6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('freq_dict.pkl','wb') as file:\n",
    "    pickle.dump(freq_dict,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b718d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sorted_list= []\n",
    "for k, v in sorted(freq_dict.items(), key=itemgetter(1),reverse=True):\n",
    "    sorted_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9688d549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'and',\n",
       " 'this',\n",
       " 'for',\n",
       " 'that',\n",
       " 'with',\n",
       " 'you',\n",
       " 'have',\n",
       " 'but',\n",
       " 'are',\n",
       " 'not',\n",
       " 'they',\n",
       " 'was',\n",
       " 'like',\n",
       " 'tast',\n",
       " 'flavor',\n",
       " 'them',\n",
       " 'these',\n",
       " 'good',\n",
       " 'tea',\n",
       " 'one',\n",
       " 'use',\n",
       " 'can',\n",
       " 'product',\n",
       " 'veri',\n",
       " 'great',\n",
       " 'just',\n",
       " 'tri',\n",
       " 'all',\n",
       " 'from',\n",
       " 'love',\n",
       " 'make',\n",
       " 'has',\n",
       " 'when',\n",
       " 'get',\n",
       " 'more',\n",
       " 'other',\n",
       " 'will',\n",
       " 'than',\n",
       " 'coffe',\n",
       " 'had',\n",
       " 'out',\n",
       " 'would',\n",
       " 'some',\n",
       " 'buy',\n",
       " 'food',\n",
       " 'onli',\n",
       " 'eat',\n",
       " 'about',\n",
       " 'time',\n",
       " 'your',\n",
       " 'find',\n",
       " 'realli',\n",
       " 'also',\n",
       " 'best',\n",
       " 'much',\n",
       " 'too',\n",
       " 'littl',\n",
       " 'order',\n",
       " 'even',\n",
       " 'amazon',\n",
       " 'becaus',\n",
       " 'drink',\n",
       " 'which',\n",
       " 'were',\n",
       " 'price',\n",
       " 'bag',\n",
       " 'there',\n",
       " 'store',\n",
       " 'been',\n",
       " 'mix',\n",
       " 'what',\n",
       " 'chocol',\n",
       " 'ani',\n",
       " 'better',\n",
       " 'well',\n",
       " 'box',\n",
       " 'sugar',\n",
       " 'now',\n",
       " 'year',\n",
       " 'their',\n",
       " 'after',\n",
       " 'sweet',\n",
       " 'found',\n",
       " 'day',\n",
       " 'dog',\n",
       " 'want',\n",
       " 'then',\n",
       " 'high',\n",
       " 'look',\n",
       " 'our',\n",
       " 'give',\n",
       " 'cup',\n",
       " 'over',\n",
       " 'first',\n",
       " 'add',\n",
       " 'water',\n",
       " 'brand',\n",
       " 'recommend',\n",
       " 'most',\n",
       " 'she',\n",
       " 'made',\n",
       " 'think',\n",
       " 'packag',\n",
       " 'way',\n",
       " 'who',\n",
       " 'treat',\n",
       " 'two',\n",
       " 'nice',\n",
       " 'work',\n",
       " 'mani',\n",
       " 'enjoy',\n",
       " 'sinc',\n",
       " 'favorit',\n",
       " 'need',\n",
       " 'thing',\n",
       " 'know',\n",
       " 'bar',\n",
       " 'keep',\n",
       " 'bit',\n",
       " 'come',\n",
       " 'differ',\n",
       " 'milk',\n",
       " 'could',\n",
       " 'purchas',\n",
       " 'say',\n",
       " 'snack',\n",
       " 'still',\n",
       " 'lot',\n",
       " 'free',\n",
       " 'delici',\n",
       " 'pack',\n",
       " 'ship',\n",
       " 'hot',\n",
       " 'her',\n",
       " 'take',\n",
       " 'never',\n",
       " 'review',\n",
       " 'organ',\n",
       " 'into',\n",
       " 'without',\n",
       " 'perfect',\n",
       " 'wonder',\n",
       " 'fresh',\n",
       " 'everi',\n",
       " 'doe',\n",
       " 'ever',\n",
       " 'befor',\n",
       " 'how',\n",
       " 'ingredi',\n",
       " 'local',\n",
       " 'sauc',\n",
       " 'cook',\n",
       " 'cat',\n",
       " 'few',\n",
       " 'alway',\n",
       " 'easi',\n",
       " 'bought',\n",
       " 'put',\n",
       " 'natur',\n",
       " 'someth',\n",
       " 'stuff',\n",
       " 'seem',\n",
       " 'cooki',\n",
       " 'it',\n",
       " 'oil',\n",
       " 'whole',\n",
       " 'healthi',\n",
       " 'green',\n",
       " 'contain',\n",
       " 'did',\n",
       " 'got',\n",
       " 'enough',\n",
       " 'hard',\n",
       " 'while',\n",
       " 'ad',\n",
       " 'right',\n",
       " 'qualiti',\n",
       " 'rice',\n",
       " 'those',\n",
       " 'same',\n",
       " 'back',\n",
       " 'regular',\n",
       " 'less',\n",
       " 'dri',\n",
       " 'last',\n",
       " 'candi',\n",
       " 'small',\n",
       " 'salt',\n",
       " 'cereal',\n",
       " 'here',\n",
       " 'calori',\n",
       " 'again',\n",
       " 'howev',\n",
       " 'long',\n",
       " 'serv',\n",
       " 'fruit',\n",
       " 'groceri',\n",
       " 'each',\n",
       " 'actual',\n",
       " 'size',\n",
       " 'tasti',\n",
       " 'quick',\n",
       " 'quit',\n",
       " 'feel',\n",
       " 'see',\n",
       " 'far',\n",
       " 'sure',\n",
       " 'old',\n",
       " 'strong',\n",
       " 'excel',\n",
       " 'definit',\n",
       " 'off',\n",
       " 'though',\n",
       " 'both',\n",
       " 'month',\n",
       " 'textur',\n",
       " 'his',\n",
       " 'peopl',\n",
       " 'chip',\n",
       " 'bread',\n",
       " 'juic',\n",
       " 'bottl',\n",
       " 'be',\n",
       " 'help',\n",
       " 'chicken',\n",
       " 'bean',\n",
       " 'problem',\n",
       " 'start',\n",
       " 'anoth',\n",
       " 'big',\n",
       " 'real',\n",
       " 'through',\n",
       " 'down',\n",
       " 'fat',\n",
       " 'smell',\n",
       " 'item',\n",
       " 'butter',\n",
       " 'open',\n",
       " 'bad',\n",
       " 'case',\n",
       " 'soup',\n",
       " 'almost',\n",
       " 'famili',\n",
       " 'blend',\n",
       " 'kid',\n",
       " 'chees',\n",
       " 'may',\n",
       " 'pretti',\n",
       " 'sever',\n",
       " 'happi',\n",
       " 'usual',\n",
       " 'per',\n",
       " 'thought',\n",
       " 'friend',\n",
       " 'should',\n",
       " 'low',\n",
       " 'go',\n",
       " 'top',\n",
       " 'black',\n",
       " 'new',\n",
       " 'bake',\n",
       " 'gluten',\n",
       " 'diet',\n",
       " 'amount',\n",
       " 'anyth',\n",
       " 'avail',\n",
       " 'varieti',\n",
       " 'compani',\n",
       " 'pasta',\n",
       " 'thank',\n",
       " 'worth',\n",
       " 'recip',\n",
       " 'light',\n",
       " 'minut',\n",
       " 'peanut',\n",
       " 'own',\n",
       " 'around',\n",
       " 'expens',\n",
       " 'onc',\n",
       " 'arriv',\n",
       " 'ice',\n",
       " 'kind',\n",
       " 'nut',\n",
       " 'home',\n",
       " 'chew',\n",
       " 'prefer',\n",
       " 'reason',\n",
       " 'full',\n",
       " 'protein',\n",
       " 'where',\n",
       " 'week',\n",
       " 'health',\n",
       " 'morn',\n",
       " 'half',\n",
       " 'especi',\n",
       " 'came',\n",
       " 'abl',\n",
       " 'dark',\n",
       " 'syrup',\n",
       " 'meal',\n",
       " 'carri',\n",
       " 'receiv',\n",
       " 'until',\n",
       " 'powder',\n",
       " 'probabl',\n",
       " 'white',\n",
       " 'spice',\n",
       " 'save',\n",
       " 'cost',\n",
       " 'expect',\n",
       " 'bitter',\n",
       " 'gift',\n",
       " 'him',\n",
       " 'leav',\n",
       " 'fill',\n",
       " 'piec',\n",
       " 'brew',\n",
       " 'pleas',\n",
       " 'honey',\n",
       " 'might',\n",
       " 'said',\n",
       " 'breakfast',\n",
       " 'such',\n",
       " 'hand',\n",
       " 'roast',\n",
       " 'star',\n",
       " 'away',\n",
       " 'vanilla',\n",
       " 'corn',\n",
       " 'call',\n",
       " 'fact',\n",
       " 'rich',\n",
       " 'place',\n",
       " 'larg',\n",
       " 'compar',\n",
       " 'extra',\n",
       " 'cream',\n",
       " 'absolut',\n",
       " 'three',\n",
       " 'instead',\n",
       " 'live',\n",
       " 'whi',\n",
       " 'coconut',\n",
       " 'sweeten',\n",
       " 'hope',\n",
       " 'cake',\n",
       " 'noth',\n",
       " 'read',\n",
       " 'wheat',\n",
       " 'type',\n",
       " 'disappoint',\n",
       " 'red',\n",
       " 'surpris',\n",
       " 'least',\n",
       " 'let',\n",
       " 'season',\n",
       " 'coupl',\n",
       " 'market',\n",
       " 'cracker',\n",
       " 'wish',\n",
       " 'slight',\n",
       " 'care',\n",
       " 'satisfi',\n",
       " 'smooth',\n",
       " 'ago',\n",
       " 'fine',\n",
       " 'decid',\n",
       " 'husband',\n",
       " 'meat',\n",
       " 'jar',\n",
       " 'end',\n",
       " 'turn',\n",
       " 'soda',\n",
       " 'second',\n",
       " 'addit',\n",
       " 'next',\n",
       " 'yet',\n",
       " 'deal',\n",
       " 'pepper',\n",
       " 'longer',\n",
       " 'chang',\n",
       " 'plus',\n",
       " 'although',\n",
       " 'stop',\n",
       " 'offer',\n",
       " 'run',\n",
       " 'includ',\n",
       " 'color',\n",
       " 'tell',\n",
       " 'went',\n",
       " 'list',\n",
       " 'must',\n",
       " 'sell',\n",
       " 'anyon',\n",
       " 'money',\n",
       " 'either',\n",
       " 'person',\n",
       " 'packet',\n",
       " 'cherri',\n",
       " 'spici',\n",
       " 'altern',\n",
       " 'fiber',\n",
       " 'noodl',\n",
       " 'orang',\n",
       " 'dish',\n",
       " 'gave',\n",
       " 'almond',\n",
       " 'glad',\n",
       " 'mayb',\n",
       " 'salad',\n",
       " 'side',\n",
       " 'appl',\n",
       " 'oliv',\n",
       " 'part',\n",
       " 'hous',\n",
       " 'sometim',\n",
       " 'amaz',\n",
       " 'stick',\n",
       " 'notic',\n",
       " 'etc',\n",
       " 'fast',\n",
       " 'soft',\n",
       " 'version',\n",
       " 'heat',\n",
       " 'seed',\n",
       " 'past',\n",
       " 'cheaper',\n",
       " 'soy',\n",
       " 'cold',\n",
       " 'everyth',\n",
       " 'origin',\n",
       " 'believ',\n",
       " 'fan',\n",
       " 'everyon',\n",
       " 'cut',\n",
       " 'els',\n",
       " 'rather',\n",
       " 'mouth',\n",
       " 'pay',\n",
       " 'pod',\n",
       " 'choic',\n",
       " 'myself',\n",
       " 'crunchi',\n",
       " 'flour',\n",
       " 'stock',\n",
       " 'hour',\n",
       " 'brown',\n",
       " 'conveni',\n",
       " 'oatmeal',\n",
       " 'experi',\n",
       " 'took',\n",
       " 'tomato',\n",
       " 'gum',\n",
       " 'potato',\n",
       " 'bowl',\n",
       " 'weight',\n",
       " 'special',\n",
       " 'ounc',\n",
       " 'lemon',\n",
       " 'close',\n",
       " 'often',\n",
       " 'popcorn',\n",
       " 'son',\n",
       " 'nutrit',\n",
       " 'direct',\n",
       " 'espresso',\n",
       " 'cinnamon',\n",
       " 'exact',\n",
       " 'total',\n",
       " 'energi',\n",
       " 'egg',\n",
       " 'clean',\n",
       " 'easili',\n",
       " 'valu',\n",
       " 'pound',\n",
       " 'grain',\n",
       " 'ask',\n",
       " 'mean',\n",
       " 'prepar',\n",
       " 'granola',\n",
       " 'consist',\n",
       " 'plain',\n",
       " 'goe',\n",
       " 'onlin',\n",
       " 'base',\n",
       " 'normal',\n",
       " 'feed',\n",
       " 'strawberri',\n",
       " 'switch',\n",
       " 'artifici',\n",
       " 'set',\n",
       " 'machin',\n",
       " 'result',\n",
       " 'vitamin',\n",
       " 'caffein',\n",
       " 'life',\n",
       " 'mild',\n",
       " 'becom',\n",
       " 'combin',\n",
       " 'near',\n",
       " 'pot',\n",
       " 'recent',\n",
       " 'particular',\n",
       " 'cours',\n",
       " 'bite',\n",
       " 'microwav',\n",
       " 'simpli',\n",
       " 'pop',\n",
       " 'cannot',\n",
       " 'stay',\n",
       " 'final',\n",
       " 'label',\n",
       " 'bodi',\n",
       " 'instant',\n",
       " 'complet',\n",
       " 'replac',\n",
       " 'carb',\n",
       " 'beef',\n",
       " 'date',\n",
       " 'ground',\n",
       " 'salti',\n",
       " 'similar',\n",
       " 'wait',\n",
       " 'certain',\n",
       " 'sodium',\n",
       " 'dure',\n",
       " 'babi',\n",
       " 'process',\n",
       " 'night',\n",
       " 'search',\n",
       " 'teeth',\n",
       " 'substitut',\n",
       " 'between',\n",
       " 'plastic',\n",
       " 'pleasant',\n",
       " 'lunch',\n",
       " 'suggest',\n",
       " 'aroma',\n",
       " 'ginger',\n",
       " 'consid',\n",
       " 'continu',\n",
       " 'shop',\n",
       " 'line',\n",
       " 'bring',\n",
       " 'cocoa',\n",
       " 'super',\n",
       " 'effect',\n",
       " 'benefit',\n",
       " 'name',\n",
       " 'check',\n",
       " 'content',\n",
       " 'fair',\n",
       " 'pick',\n",
       " 'four',\n",
       " 'daughter',\n",
       " 'veget',\n",
       " 'except',\n",
       " 'servic',\n",
       " 'chewi',\n",
       " 'bulk',\n",
       " 'gram',\n",
       " 'finish',\n",
       " 'deliv',\n",
       " 'smaller',\n",
       " 'pancak',\n",
       " 'extrem',\n",
       " 'chai',\n",
       " 'rate',\n",
       " 'oat',\n",
       " 'daili',\n",
       " 'along',\n",
       " 'individu',\n",
       " 'pure',\n",
       " 'addict',\n",
       " 'guess',\n",
       " 'rememb',\n",
       " 'state',\n",
       " 'fish',\n",
       " 'left',\n",
       " 'point',\n",
       " 'eaten',\n",
       " 'provid',\n",
       " 'sit',\n",
       " 'pill',\n",
       " 'yummi',\n",
       " 'berri',\n",
       " 'christma',\n",
       " 'note',\n",
       " 'delight',\n",
       " 'entir',\n",
       " 'overal',\n",
       " 'given',\n",
       " 'formula',\n",
       " 'decaf',\n",
       " 'custom',\n",
       " 'singl',\n",
       " 'gummi',\n",
       " 'follow',\n",
       " 'someon',\n",
       " 'itself',\n",
       " 'jerki',\n",
       " 'wife',\n",
       " 'glass',\n",
       " 'acid',\n",
       " 'area',\n",
       " 'alreadi',\n",
       " 'carbon',\n",
       " 'deliveri',\n",
       " 'raw',\n",
       " 'insid',\n",
       " 'pet',\n",
       " 'huge',\n",
       " 'interest',\n",
       " 'fantast',\n",
       " 'coat',\n",
       " 'world',\n",
       " 'mint',\n",
       " 'thick',\n",
       " 'bold',\n",
       " 'sold',\n",
       " 'later',\n",
       " 'french',\n",
       " 'pour',\n",
       " 'supermarket',\n",
       " 'aftertast',\n",
       " 'idea',\n",
       " 'mine',\n",
       " 'truli',\n",
       " 'refresh',\n",
       " 'wrong',\n",
       " 'licoric',\n",
       " 'discov',\n",
       " 'chili',\n",
       " 'watch',\n",
       " 'toy',\n",
       " 'roll',\n",
       " 'throw',\n",
       " 'beverag',\n",
       " 'cover',\n",
       " 'import',\n",
       " 'garlic',\n",
       " 'dinner',\n",
       " 'larger',\n",
       " 'boil',\n",
       " 'true',\n",
       " 'due',\n",
       " 'hold',\n",
       " 'creami',\n",
       " 'share',\n",
       " 'sourc',\n",
       " 'liquid',\n",
       " 'consum',\n",
       " 'ate',\n",
       " 'hit',\n",
       " 'saw',\n",
       " 'togeth',\n",
       " 'allergi',\n",
       " 'rest',\n",
       " 'caus',\n",
       " 'wast',\n",
       " 'preserv',\n",
       " 'dress',\n",
       " 'hint',\n",
       " 'maker',\n",
       " 'restaur',\n",
       " 'yes',\n",
       " 'issu',\n",
       " 'miss',\n",
       " 'mind',\n",
       " 'general',\n",
       " 'five',\n",
       " 'balanc',\n",
       " 'produc',\n",
       " 'clear',\n",
       " 'gone',\n",
       " 'break',\n",
       " 'slice',\n",
       " 'sour',\n",
       " 'sale',\n",
       " 'herb',\n",
       " 'difficult',\n",
       " 'sent',\n",
       " 'fun',\n",
       " 'told',\n",
       " 'suppos',\n",
       " 'lower',\n",
       " 'melt',\n",
       " 'mention',\n",
       " 'warm',\n",
       " 'loos',\n",
       " 'simpl',\n",
       " 'done',\n",
       " 'within',\n",
       " 'drop',\n",
       " 'return',\n",
       " 'worri',\n",
       " 'stomach',\n",
       " 'unfortun',\n",
       " 'crunch',\n",
       " 'veggi',\n",
       " 'awesom',\n",
       " 'plant',\n",
       " 'option',\n",
       " 'dip',\n",
       " 'bottom',\n",
       " 'beat',\n",
       " 'anyway',\n",
       " 'wrap',\n",
       " 'excit',\n",
       " 'crave',\n",
       " 'amazoncom',\n",
       " 'impress',\n",
       " 'tradit',\n",
       " 'healthier',\n",
       " 'plan',\n",
       " 'thin',\n",
       " 'starbuck',\n",
       " 'batch',\n",
       " 'easier',\n",
       " 'allow',\n",
       " 'seen',\n",
       " 'picki',\n",
       " 'unlik',\n",
       " 'onion',\n",
       " 'level',\n",
       " 'anywher',\n",
       " 'shape',\n",
       " 'bone',\n",
       " 'under',\n",
       " 'vet',\n",
       " 'raisin',\n",
       " 'twice',\n",
       " 'banana',\n",
       " 'grow',\n",
       " 'fit',\n",
       " 'figur',\n",
       " 'drinker',\n",
       " 'immedi',\n",
       " 'avoid',\n",
       " 'fri',\n",
       " 'beauti',\n",
       " 'soon',\n",
       " 'yogurt',\n",
       " 'happen',\n",
       " 'stir',\n",
       " 'busi',\n",
       " 'test',\n",
       " 'grey',\n",
       " 'opinion',\n",
       " 'kitchen',\n",
       " 'diabet',\n",
       " 'yourself',\n",
       " 'possibl',\n",
       " 'tuna',\n",
       " 'toast',\n",
       " 'italian',\n",
       " 'crisp',\n",
       " 'websit',\n",
       " 'show',\n",
       " 'unless',\n",
       " 'number',\n",
       " 'sort',\n",
       " 'none',\n",
       " 'tend',\n",
       " 'chemic',\n",
       " 'blueberri',\n",
       " 'cool',\n",
       " 'form',\n",
       " 'requir',\n",
       " 'seal',\n",
       " 'hate',\n",
       " 'moist',\n",
       " 'sandwich',\n",
       " 'refriger',\n",
       " 'children',\n",
       " 'remind',\n",
       " 'skin',\n",
       " 'concern',\n",
       " 'do',\n",
       " 'vinegar',\n",
       " 'sampl',\n",
       " 'send',\n",
       " 'uniqu',\n",
       " 'digest',\n",
       " 'six',\n",
       " 'matter',\n",
       " 'browni',\n",
       " 'homemad',\n",
       " 'shake',\n",
       " 'today',\n",
       " 'perhap',\n",
       " 'tin',\n",
       " 'stand',\n",
       " 'pie',\n",
       " 'cheap',\n",
       " 'mapl',\n",
       " 'basic',\n",
       " 'site',\n",
       " 'medium',\n",
       " 'manufactur',\n",
       " 'agre',\n",
       " 'wine',\n",
       " 'blue',\n",
       " 'describ',\n",
       " 'subscrib',\n",
       " 'short',\n",
       " 'trip',\n",
       " 'blood',\n",
       " 'earl',\n",
       " 'condit',\n",
       " 'touch',\n",
       " 'keurig',\n",
       " 'suppli',\n",
       " 'flower',\n",
       " 'lover',\n",
       " 'main',\n",
       " 'shipment',\n",
       " 'tini',\n",
       " 'extract',\n",
       " 'trap',\n",
       " 'standard',\n",
       " 'sprinkl',\n",
       " 'typic',\n",
       " 'previous',\n",
       " 'stevia',\n",
       " 'biscuit',\n",
       " 'gourmet',\n",
       " 'steep',\n",
       " 'tart',\n",
       " 'appreci',\n",
       " 'splenda',\n",
       " 'expir',\n",
       " 'broken',\n",
       " 'hook',\n",
       " 'improv',\n",
       " 'realiz',\n",
       " 'muffin',\n",
       " 'readi',\n",
       " 'american',\n",
       " 'bland',\n",
       " 'concentr',\n",
       " 'instruct',\n",
       " 'travel',\n",
       " 'complaint',\n",
       " 'mill',\n",
       " 'move',\n",
       " 'anymor',\n",
       " 'imagin',\n",
       " 'overpow',\n",
       " 'raspberri',\n",
       " 'tree',\n",
       " 'higher',\n",
       " 'bear',\n",
       " 'remov',\n",
       " 'word',\n",
       " 'somewhat',\n",
       " 'research',\n",
       " 'quantiti',\n",
       " 'portion',\n",
       " 'reduc',\n",
       " 'visit',\n",
       " 'mom',\n",
       " 'dessert',\n",
       " 'whatev',\n",
       " 'offic',\n",
       " 'shelf',\n",
       " 'incred',\n",
       " 'caramel',\n",
       " 'english',\n",
       " 'doubl',\n",
       " 'count',\n",
       " 'creat',\n",
       " 'pan',\n",
       " 'otherwis',\n",
       " 'pocket',\n",
       " 'sound',\n",
       " 'learn',\n",
       " 'straight',\n",
       " 'seller',\n",
       " 'pizza',\n",
       " 'leaf',\n",
       " 'decent',\n",
       " 'stale',\n",
       " 'frozen',\n",
       " 'broth',\n",
       " 'subtl',\n",
       " 'tablespoon',\n",
       " 'sea',\n",
       " 'heavi',\n",
       " 'thrill',\n",
       " 'spoon',\n",
       " 'grape',\n",
       " 'waffl',\n",
       " 'kept',\n",
       " 'spread',\n",
       " 'claim',\n",
       " 'senseo',\n",
       " 'lose',\n",
       " 'summer',\n",
       " 'cashew',\n",
       " 'pictur',\n",
       " 'weak',\n",
       " 'brought',\n",
       " 'ball',\n",
       " 'forward',\n",
       " 'dollar',\n",
       " 'salmon',\n",
       " 'parti',\n",
       " 'control',\n",
       " 'system',\n",
       " 'chunk',\n",
       " 'train',\n",
       " 'kick',\n",
       " 'mess',\n",
       " 'curri',\n",
       " 'serious',\n",
       " 'flake',\n",
       " 'puppi',\n",
       " 'spend',\n",
       " 'across',\n",
       " 'terribl',\n",
       " 'mustard',\n",
       " 'vegan',\n",
       " 'increas',\n",
       " 'knew',\n",
       " 'felt',\n",
       " 'appear',\n",
       " 'charg',\n",
       " 'inform',\n",
       " 'various',\n",
       " 'pricey',\n",
       " 'chanc',\n",
       " 'sick',\n",
       " 'plenti',\n",
       " 'lack',\n",
       " 'chop',\n",
       " 'hazelnut',\n",
       " 'room',\n",
       " 'outsid',\n",
       " 'stronger',\n",
       " 'teaspoon',\n",
       " 'alon',\n",
       " 'known',\n",
       " 'age',\n",
       " 'herbal',\n",
       " 'afternoon',\n",
       " 'relat',\n",
       " 'fall',\n",
       " 'heart',\n",
       " 'heard',\n",
       " 'freez',\n",
       " 'yeast',\n",
       " 'vegetarian',\n",
       " 'present',\n",
       " 'play',\n",
       " 'troubl',\n",
       " 'job',\n",
       " 'comment',\n",
       " 'paper',\n",
       " 'descript',\n",
       " 'crust',\n",
       " 'celiac',\n",
       " 'power',\n",
       " 'style',\n",
       " 'smoke',\n",
       " 'pouch',\n",
       " 'whether',\n",
       " 'jelli',\n",
       " 'grill',\n",
       " 'burn',\n",
       " 'premium',\n",
       " 'late',\n",
       " 'sensit',\n",
       " 'prompt',\n",
       " 'fridg',\n",
       " 'mother',\n",
       " 'crazi',\n",
       " 'pretzel',\n",
       " 'book',\n",
       " 'school',\n",
       " 'pass',\n",
       " 'crispi',\n",
       " 'everyday',\n",
       " 'paid',\n",
       " 'indian',\n",
       " 'aw',\n",
       " 'understand',\n",
       " 'nutti',\n",
       " 'child',\n",
       " 'grind',\n",
       " 'safe',\n",
       " 'quinoa',\n",
       " 'introduc',\n",
       " 'sticki',\n",
       " 'allerg',\n",
       " 'supplement',\n",
       " 'agav',\n",
       " 'remain',\n",
       " 'equal',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d01f8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can rememb see the show when air televis year ago when was child sister later bought the which have this day thirti somethingi use this seri book song when did student teach for preschool turn the whole school now purchas along with the book for children the tradit live'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84cc63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words= 5000\n",
    "sorted_list= sorted_list[:5000]\n",
    "import pickle\n",
    "file_name = \"sortedlist.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(sorted_list, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be1d38a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "column=[]\n",
    "for sent in Data_x:\n",
    "    lis=[]\n",
    "    for word in sent.split():\n",
    "        if word in sorted_list:\n",
    "            lis.append(word)\n",
    "    column.append(' '.join(lis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cd00cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('column.pkl','wb') as file:\n",
    "    pickle.dump(column,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e93247db",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_x=[]\n",
    "for sent in Data_x:\n",
    "    lis=[]\n",
    "    for word in sent.split():\n",
    "        if word in sorted_list:\n",
    "            lis.append(sorted_list.index(word)+1)\n",
    "    final_x.append(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7768539",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest= final_x[:30000]\n",
    "Ytest= Data_y[:30000]\n",
    "Xtrain= final_x[30000:]\n",
    "Ytrain= Data_y[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2419933f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 216, 209, 1977, 106, 8, 368, 25, 3968, 527, 3224, 3224, 721, 17, 488, 1100, 2, 583, 127, 5, 1069, 474, 5, 11, 698, 8, 47, 28, 1, 557, 16, 3224, 3224, 9, 322, 1, 31, 89, 99, 12, 10, 156, 621, 42, 91, 151, 69, 541, 35, 1133, 17, 61, 5, 8, 352, 1, 69]\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd3fa7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_review_length=600\n",
    "Xtrain = sequence.pad_sequences(Xtrain, maxlen=max_review_length)\n",
    "Xtest= sequence.pad_sequences(Xtest, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a47b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    8  216  209\n",
      " 1977  106    8  368   25 3968  527 3224 3224  721   17  488 1100    2\n",
      "  583  127    5 1069  474    5   11  698    8   47   28    1  557   16\n",
      " 3224 3224    9  322    1   31   89   99   12   10  156  621   42   91\n",
      "  151   69  541   35 1133   17   61    5    8  352    1   69]\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89a04a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5642568739446880652\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1119089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24ab8ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 600, 32)           160032    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,333\n",
      "Trainable params: 213,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "# building the sequential model\n",
    "model = Sequential()\n",
    "# adding the enbedded layer\n",
    "model.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\n",
    "# adding the LSTM\n",
    "model.add(LSTM(100))\n",
    "# activation function is choosen as sigmoid as we havetwo different output values\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# configuring the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c39ec83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1094/1094 [==============================] - 630s 574ms/step - loss: 0.2757 - accuracy: 0.8948\n",
      "Epoch 2/10\n",
      "1094/1094 [==============================] - 706s 645ms/step - loss: 0.1923 - accuracy: 0.9240\n",
      "Epoch 3/10\n",
      "1094/1094 [==============================] - 781s 713ms/step - loss: 0.1783 - accuracy: 0.9308\n",
      "Epoch 4/10\n",
      "1094/1094 [==============================] - 757s 692ms/step - loss: 0.1554 - accuracy: 0.9398\n",
      "Epoch 5/10\n",
      "1094/1094 [==============================] - 716s 654ms/step - loss: 0.1423 - accuracy: 0.9444\n",
      "Epoch 6/10\n",
      "1094/1094 [==============================] - 716s 654ms/step - loss: 0.1304 - accuracy: 0.9497\n",
      "Epoch 7/10\n",
      "1094/1094 [==============================] - 747s 683ms/step - loss: 0.1200 - accuracy: 0.9539\n",
      "Epoch 8/10\n",
      "1094/1094 [==============================] - 608s 556ms/step - loss: 0.1053 - accuracy: 0.9605\n",
      "Epoch 9/10\n",
      "1094/1094 [==============================] - 727s 664ms/step - loss: 0.0921 - accuracy: 0.9664\n",
      "Epoch 10/10\n",
      "1094/1094 [==============================] - 766s 700ms/step - loss: 0.0802 - accuracy: 0.9709\n",
      "Accuracy: 93.52%\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(Xtrain, Ytrain, epochs=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(Xtest, Ytest, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2026ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "from keras.models import load_model\n",
    "model.save('model.h5')\n",
    "# saving the model\n",
    "model1 = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afaf4d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 600, 32)           160032    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 600, 100)          53200     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 600, 100)          0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 80)                57920     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,233\n",
      "Trainable params: 271,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model with adding dropout \n",
    "embedding_vecor_length = 32\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\n",
    "model2.add(LSTM(100,return_sequences=True))\n",
    "model2.add(Dropout(0.25))\n",
    "model2.add(LSTM(80))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e89c58fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1094/1094 [==============================] - 1521s 1s/step - loss: 0.2398 - accuracy: 0.9095\n",
      "Epoch 2/10\n",
      "1094/1094 [==============================] - 1636s 1s/step - loss: 0.1793 - accuracy: 0.9325\n",
      "Epoch 3/10\n",
      "1094/1094 [==============================] - 1689s 2s/step - loss: 0.1536 - accuracy: 0.9425\n",
      "Epoch 4/10\n",
      "1094/1094 [==============================] - 1700s 2s/step - loss: 0.1359 - accuracy: 0.9495\n",
      "Epoch 5/10\n",
      "1094/1094 [==============================] - 1774s 2s/step - loss: 0.1184 - accuracy: 0.9575\n",
      "Epoch 6/10\n",
      "1094/1094 [==============================] - 1739s 2s/step - loss: 0.1027 - accuracy: 0.9631\n",
      "Epoch 7/10\n",
      "1094/1094 [==============================] - 1713s 2s/step - loss: 0.0980 - accuracy: 0.9639\n",
      "Epoch 8/10\n",
      "1094/1094 [==============================] - 1608s 1s/step - loss: 0.0840 - accuracy: 0.9706\n",
      "Epoch 9/10\n",
      "1094/1094 [==============================] - 1622s 1s/step - loss: 0.0695 - accuracy: 0.9758\n",
      "Epoch 10/10\n",
      "1094/1094 [==============================] - 1634s 1s/step - loss: 0.0619 - accuracy: 0.9790\n",
      "Accuracy: 93.30%\n"
     ]
    }
   ],
   "source": [
    "history2= model2.fit(Xtrain, Ytrain, epochs=10, batch_size=64)# Final evaluation of the model\n",
    "scores = model2.evaluate(Xtest, Ytest, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f6bd7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model2.save('model2.h5')\n",
    "model3 = load_model('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model.pkl','wb') as file:\n",
    "#     pickle.dump(model,file)\n",
    "# with open('model2.pkl','wb') as file:\n",
    "#     pickle.dump(model2,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdbbe114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a function which convert sentance to requred vectorized format that will feed well in model\n",
    "\n",
    "def cleanhtml(sentance): #substitute expression contained in <> with ' '\n",
    "    cleaned= re.sub(re.compile('<.*?>'),' ',sentance)\n",
    "    return cleaned\n",
    "#function for removing punctuations chars\n",
    "def cleanpunc(sentance):\n",
    "    cleaned= re.sub(r'[?|!|\\'|\"|#]',r'',sentance)\n",
    "    cleaned= re.sub(r'[.|,|)|(|\\|/]',r'',sentance)\n",
    "    return cleaned\n",
    "snowstemmer= sno('english')\n",
    "\n",
    "def predict_this(sentance):\n",
    "    i=0\n",
    "    str1=' '\n",
    "    final_string=[]\n",
    "    all_positive_words=[] # store words from +ve reviews here\n",
    "    all_negative_words=[] # store words from -ve reviews here.\n",
    "    sent= sentance\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\n",
    "    sent=cleanhtml(sent) # remove HTMl tags\n",
    "    for w in sent.split():\n",
    "        # we have used cleanpunc(w).split(), one more split function here \n",
    "        # because consider w=\"abc.def\", cleanpunc(w) will return \"abc def\"\n",
    "        # if we dont use .split() function then we will be considring \"abc def\" \n",
    "        # as a single word, but if you use .split() function we will get \"abc\", \"def\"\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                s=(snowstemmer.stem(cleaned_words.lower())).encode('utf8')\n",
    "                filtered_sentence.append(s)\n",
    "                if(data['Score'].values)[i] =='Positive':\n",
    "                    all_positive_words.append(s)\n",
    "                if(data['Score'].values)[i] =='Negative':\n",
    "                    all_negative_words.append(s)\n",
    "            else:\n",
    "                continue\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    final_string.append(str1)\n",
    "\n",
    "    final_string\n",
    "    for i in final_string:\n",
    "        final_string=i.decode(\"utf-8\")\n",
    "\n",
    "    lis=[]\n",
    "    for word in final_string.split():\n",
    "        if word in sorted_list:\n",
    "            lis.append(sorted_list.index(word)+1)\n",
    "\n",
    "    final_string= lis\n",
    "    final_string = sequence.pad_sequences([final_string], maxlen=max_review_length)\n",
    "    print(final_string)\n",
    "    what= ''\n",
    "    if (round(float(model3.predict(final_string)))==1):\n",
    "        what= 'Positive'\n",
    "        acc= round(float(model3.predict(final_string))*100,2)\n",
    "    else:\n",
    "        what='Negative'\n",
    "        acc= 100- round(float(model3.predict(final_string))*100,2)\n",
    "    print(what,'review with',acc,'% Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aee0496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  46  13  25 240  15]]\n",
      "Negative review with 99.03 % Accuracy\n"
     ]
    }
   ],
   "source": [
    "sentance= 'food was very bad in taste'\n",
    "predict_this(sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7296ad96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  46  13 800 202]]\n",
      "Positive review with 86.45 % Accuracy\n"
     ]
    }
   ],
   "source": [
    "sentance= 'food was medium tasty'\n",
    "predict_this(sentance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
